import requests
from bs4 import BeautifulSoup
import re
import time
from yaspin import yaspin
from yaspin.spinners import Spinners
import concurrent.futures
import logging
from rich.console import Console
from rich.panel import Panel
from rich.text import Text
from rich.align import Align
from rich.style import Style
import subprocess
import sys
import importlib
import colorama
import random

console = Console()
logging.basicConfig(filename='proxy_scraper.log', level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')
colorama.init(autoreset=True)

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0",
    "Mozilla/5.0 (iPad; CPU OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Mobile/15E148 Safari/604.1"
]

def install_missing_libraries():
    libraries_to_check = ["requests", "beautifulsoup4", "rich", "yaspin", "concurrent.futures", "colorama"]
    missing_libraries = []

    console.print("[cyan]üîç Ki·ªÉm tra th∆∞ vi·ªán c·∫ßn thi·∫øt...[/cyan]")

    try:
        import importlib
        for library in libraries_to_check:
            if not importlib.util.find_spec(library):
                missing_libraries.append(library)

        if missing_libraries:
            console.print(f"[yellow]‚ö†Ô∏è  Thi·∫øu c√°c th∆∞ vi·ªán: [bold yellow]{', '.join(missing_libraries)}[/]. [cyan]B·∫Øt ƒë·∫ßu c√†i ƒë·∫∑t...[/cyan]")
            for library in missing_libraries:
                console.print(f"[cyan]‚è≥ ƒêang c√†i ƒë·∫∑t {library}...[/cyan]", end="")
                try:
                    subprocess.check_call([sys.executable, "-m", "pip", "install", library],  stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                    console.print(f"\r[green]‚úÖ ƒê√£ c√†i ƒë·∫∑t {library}.[/]")
                except subprocess.CalledProcessError as e:
                    console.print(f"\r[red]‚ùå L·ªói c√†i ƒë·∫∑t {library}: {str(e)}.[/] [yellow]Vui l√≤ng c√†i ƒë·∫∑t th·ªß c√¥ng.[/]")
            console.print("[green]‚úÖ Ho√†n t·∫•t c√†i ƒë·∫∑t.[/]")
        else:
            console.print("[green]‚úÖ T·∫•t c·∫£ th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t ƒë·∫ßy ƒë·ªß. üéâ[/]")

    except Exception as e:
        console.print(f"[red]‚ùå L·ªói ki·ªÉm tra v√† c√†i ƒë·∫∑t th∆∞ vi·ªán: {str(e)}.[/] [yellow]Vui l√≤ng ki·ªÉm tra Python.[/]")
        logging.error(f"L·ªói ki·ªÉm tra v√† c√†i ƒë·∫∑t th∆∞ vi·ªán: {e}")


def test_proxy(proxy, protocol="https", timeout=15):
    try:
        if protocol == "https":
            requests.get("https://www.google.com", proxies={"https": proxy, "http": proxy}, timeout=timeout)
        elif protocol == "http":
            requests.get("http://httpbin.org/ip", proxies={"http": proxy}, timeout=timeout)
        return True
    except Exception as e:
        logging.error(f"L·ªói ki·ªÉm tra proxy {proxy} ({protocol}): {e}")
        return False

def check_proxies_multithreaded(proxy_list, protocol="https", num_threads=400):
    live_proxies = []
    dead_proxies = []
    with yaspin(Spinners.line, text=f"ƒêang ki·ªÉm tra proxy {protocol.upper()} ƒëa lu·ªìng...") as spinner_test:
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = [executor.submit(test_proxy, proxy, protocol) for proxy in proxy_list]
            for index, future in enumerate(concurrent.futures.as_completed(futures)):
                proxy = proxy_list[index]
                if future.result():
                    live_proxies.append(proxy)
                else:
                    dead_proxies.append(proxy)
        spinner_test.text = f"ƒê√£ ki·ªÉm tra xong proxy {protocol.upper()}."
        spinner_test.ok("‚úî")
    return live_proxies, dead_proxies

def scrape_source(source_info):
    source_name, source_url = source_info
    http_proxies_source = []
    https_proxies_source = []
    console.print(f"[cyan][bold]Ngu·ªìn VIP:[/bold][/cyan] [yellow]{source_name}[/]")
    with yaspin(Spinners.earth, text=f"{colorama.Fore.CYAN}ƒêang x·ª≠ l√Ω...{colorama.Style.RESET_ALL}") as spinner:
        try:
            random_user_agent = random.choice(USER_AGENTS)
            response = requests.get(source_url, headers={'User-Agent': random_user_agent}, timeout=15)
            response.raise_for_status()

            if "proxy-list.download" in source_url:
                for line in response.text.splitlines():
                    if re.match(r"^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d+$", line.strip()):
                        https_proxies_source.append(line.strip())

            elif "githubusercontent.com" in source_url or "proxyspace.pro" in source_url or "openproxy.space" in source_url:
                for line in response.text.splitlines():
                    if re.match(r"^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d+$", line.strip()):
                        https_proxies_source.append(line.strip())

            elif "freeproxychecker.com" in source_url:
                soup = BeautifulSoup(response.text, 'html.parser')
                table = soup.find('table', class_='table table-striped')
                if table:
                    for row in table.find_all('tr')[1:]:
                        cols = row.find_all('td')
                        if len(cols) >= 2:
                            ip_address = cols[0].text.strip()
                            port = cols[1].text.strip()
                            https_proxies_source.append(f"{ip_address}:{port}")

            elif "my-proxy.com" in source_url or "us-proxy.org" in source_url or "free-proxy-list.net" in source_url or "uk-proxy.html" in source_url or "socks-proxy.net" in source_url or "proxy-list.net" in source_url:
                soup = BeautifulSoup(response.text, 'html.parser')
                table = soup.find('table')
                if table:
                    for row in table.find_all('tr')[1:]:
                        cols = row.find_all('td')
                        if len(cols) >= 2:
                            ip_address = cols[0].text.strip()
                            port = cols[1].text.strip()
                            protocol_col = cols[6].text.strip().lower() if len(cols) > 6 else ""
                            proxy_type = cols[4].text.strip().lower() if len(cols) > 4 else ""

                            if "https" in protocol_col or "yes" in protocol_col or "elite proxy" in protocol_col.lower() or "anonymous" in protocol_col.lower() or "ssl" in proxy_type:
                                https_proxies_source.append(f"{ip_address}:{port}")
                            elif "http" in protocol_col or "transparent" in proxy_type or "no" in protocol_col.lower() or "http" in proxy_type:
                                http_proxies_source.append(f"{ip_address}:{port}")
                            else:
                                https_proxies_source.append(f"{ip_address}:{port}")

            spinner.stop()
            console.print(f"[green][bold]ƒê√£ l·∫•y:[/bold][/green] [white bold]{len(http_proxies_source) + len(https_proxies_source)}[/white bold] [green]proxy t·ª´:[/green] [yellow]{source_name}[/]")
            return http_proxies_source, https_proxies_source

        except requests.exceptions.RequestException as e:
            spinner.stop()
            console.print(f"[red][bold]L·ªói truy c·∫≠p:[/bold][/red] [yellow]{source_name}[/]: [bold red]{e}[/]")
            logging.error(f"L·ªói RequestException khi truy c·∫≠p {source_url}: {e}")
        except Exception as e:
            spinner.stop()
            console.print(f"[red][bold]L·ªói x·ª≠ l√Ω:[/bold][/red] [yellow]{source_name}[/]: [bold red]{e}[/]")
            logging.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi x·ª≠ l√Ω {source_url}: {e}")
        return [], []

def get_proxies(selected_source_indices=None, sources=None):
    if sources is None:
        return [], []

    sources_to_scrape = []
    if selected_source_indices:
        for index in selected_source_indices:
            if 0 <= index < len(sources):
                sources_to_scrape.append(sources[index])
            else:
                console.print(f"[red]C·∫£nh b√°o:[/red] Ngu·ªìn kh√¥ng h·ª£p l·ªá: [bold red]{index + 1}[/]. [yellow]B·ªè qua.[/yellow]")
    else:
        sources_to_scrape = sources

    http_proxies = []
    https_proxies = []

    if not sources_to_scrape:
        console.print("[yellow]Kh√¥ng c√≥ ngu·ªìn proxy n√†o ƒë∆∞·ª£c ch·ªçn ho·∫∑c h·ª£p l·ªá.[/]")
        return [], []

    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        source_results = executor.map(scrape_source, sources_to_scrape)

        for http_source, https_source in source_results:
            http_proxies.extend(http_source)
            https_proxies.extend(https_source)

    http_proxies = list(set(filter(None, http_proxies)))
    https_proxies = list(set(filter(None, https_proxies)))
    return http_proxies, https_proxies

def save_proxies_to_files(http_proxies, https_proxies, type="all"):
    if not http_proxies and not https_proxies:
        console.print("[red]Kh√¥ng c√≥ proxy n√†o ƒë·ªÉ l∆∞u.[/]")
        return False

    try:
        if http_proxies and type in ("all", "http"):
            with open("http_proxies.txt", 'w') as f_http:
                console.print("[cyan]ƒêang l∆∞u HTTP proxy:[/] [yellow]http_proxies.txt[/]")
                for proxy in http_proxies:
                    f_http.write(proxy + '\n')
            console.print("[green]ƒê√£ l∆∞u HTTP proxy v√†o:[/] [yellow]http_proxies.txt[/]")

        if https_proxies and type in ("all", "https"):
            with open("https_proxies.txt", 'w') as f_https:
                console.print("[cyan]ƒêang l∆∞u HTTPS proxy:[/] [yellow]https_proxies.txt[/]")
                for proxy in https_proxies:
                    f_https.write(proxy + '\n')
            console.print("[green]ƒê√£ l∆∞u HTTPS proxy v√†o:[/] [yellow]https_proxies.txt[/]")

        if http_proxies and type == "live": 
					
            with open("live_http_proxies.txt", 'w') as f_live_http:
                console.print("[cyan]ƒêang l∆∞u LIVE HTTP proxy:[/] [yellow]live_http_proxies.txt[/]")
                for proxy in http_proxies:
                    f_live_http.write(proxy + '\n')
            console.print("[green]ƒê√£ l∆∞u LIVE HTTP proxy v√†o:[/] [yellow]live_http_proxies.txt[/]")

        if https_proxies and type == "live":
            with open("live_https_proxies.txt", 'w') as f_live_https:
                console.print("[cyan]ƒêang l∆∞u LIVE HTTPS proxy:[/] [yellow]live_https_proxies.txt[/]")
                for proxy in https_proxies:
                    f_live_https.write(proxy + '\n')
            console.print("[green]ƒê√£ l∆∞u LIVE HTTPS proxy v√†o:[/] [yellow]live_https_proxies.txt[/]")


        console.print("[green]ƒê√£ l∆∞u danh s√°ch proxy v√†o file.[/]")
        return True
    except Exception as e:
        console.print(f"[red]L·ªói l∆∞u file:[/] [bold red]{e}[/]")
        logging.error(f"L·ªói khi l∆∞u file proxy: {e}")
        return False

def display_proxy_lists(http_proxies, https_proxies):
    if http_proxies:
        console.print(Panel(Align.center(Text("Danh s√°ch Proxy HTTP VIP", style="bold green")), border_style="green", title="[bold green]HTTP Proxies[/]"))
        for proxy in http_proxies:
            console.print(f"[green]‚úÖ [LIVE - HTTP][/] [white]{proxy}[/]")
        console.print(f"\n[green]T·ªïng c·ªông HTTP s·ªëng:[/] [white bold]{len(http_proxies)}[/]")

    if https_proxies:
        console.print(Panel(Align.center(Text("Danh s√°ch Proxy HTTPS VIP", style="bold green")), border_style="green", title="[bold green]HTTPS Proxies[/]"))
        for proxy in https_proxies:
            console.print(f"[green]‚úÖ [LIVE - HTTPS][/] [white]{proxy}[/]")
        console.print(f"\n[green]T·ªïng c·ªông HTTPS VIP:[/] [white bold]{len(https_proxies)}[/]")

    if not http_proxies and not https_proxies:
        console.print("[red]Kh√¥ng t√¨m th·∫•y proxy VIP n√†o.[/]")

def main_menu():
    install_missing_libraries()

    sources = [
        ("ProxyEmpire VIP (HTTPS) üëë", "https://proxyempire.io/api/v2/proxies?protocol=https&anonymityLevel=elite&country=us,gb,de,fr,nl,ca"),
        ("ProxySeller VIP (HTTPS) üíé", "https://proxy-seller.com/api/proxy?type=https&anonymity=elite&country=us,gb,de,fr,nl,ca"),
        ("Webshare VIP (HTTPS) ü•á", "https://webshare.io/api/proxy/list/download?protocol=https&anonymity=elite&country_codes=US,GB,DE,FR,NL,CA"),
        ("Infatica VIP (HTTPS) üèÜ", "https://infatica.io/api/v2/proxy/list?protocol=https&anonymity=elite&country=us,gb,de,fr,nl,ca"),
        ("NetNut VIP (HTTPS) üõ°Ô∏è", "https://netnut.io/api/proxies?protocol=https&anonymity=elite&country=us,gb,de,fr,nl,ca"),
        ("Smartproxy VIP (HTTPS)", "https://smartproxy.com/api/v1/proxies?protocol=https&anonymity=elite&country=us,gb,de,fr,nl,ca"),
        ("Oxylabs VIP (HTTPS)", "https://oxylabs.io/api/proxies?protocol=https&anonymity=elite&country=us,gb,de,fr,nl,ca"),
        ("Bright Data VIP (HTTPS)", "https://brightdata.com/api/proxies?protocol=https&anonymity=elite&country=us,gb,de,fr,nl,ca"),
        ("Crawlera VIP (HTTPS)", "https://crawlera.com/api/proxies?protocol=https&anonymity=elite&country=us,gb,de,fr,nl,ca"),
        ("Zyte VIP (HTTPS)", "https://www.zyte.com/api/proxies?protocol=https&anonymity=elite&country=us,gb,de,fr,nl,ca"),
        ("IpremiumWeb VIP (HTTPS)", "https://www.ipremiumweb.com/api/v2/proxies?protocol=https&anonymityLevel=elite&country=us,gb,de,fr,nl,ca"),
        ("Storm Proxies VIP (HTTPS)", "https://stormproxies.com/api/v2/proxies?protocol=https&anonymityLevel=elite&country=us,gb,de,fr,nl,ca"),
        ("SOAX VIP (HTTPS)", "https://soax.com/api/v2/proxies?protocol=https&anonymityLevel=elite&country=us,gb,de,fr,nl,ca"),
        ("Hydraproxy VIP (HTTPS)", "https://hydraproxy.com/api/v2/proxies?protocol=https&anonymityLevel=elite&country=us,gb,de,fr,nl,ca"),
        ("Web-Proxy.net (Free HTTPS)", "https://web-proxy.net/freeproxy/https"),
        ("Free-Proxy-List.net (HTTPS)", "https://free-proxy-list.net/"),
        ("SSLProxies.org (HTTPS)", "https://www.sslproxies.org/"),
        ("Proxy-List.download (HTTPS API)", "https://www.proxy-list.download/api/v1/get?type=https"),
        ("Proxy-List.download (HTTP API)", "https://www.proxy-list.download/api/v1/get?type=http"),
        ("PROXY-List (Github - HTTP)", "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt"),
        ("Proxy-List (Github - HTTPS)", "https://raw.githubusercontent.com/ShiftyTR/Proxy-List/master/https.txt"),
        ("Pro Xy Space (HTTPS)", "https://proxyspace.pro/https.txt"),
        ("Open Proxy Space (HTTPS)", "https://openproxy.space/list/https"),
        ("FreeProxyChecker (HTTPS Elite)", "https://www.freeproxychecker.com/result/?protocol=https&anonymity=elite"),
        ("My-Proxy (Anonymous)", "https://www.my-proxy.com/free-anonymous-proxy.html"),
        ("US Proxy (Anonymous)", "https://www.us-proxy.org/"),
        ("Free-Proxy-List.net (UK)", "https://free-proxy-list.net/uk-proxy.html"),
        ("Proxy-List.net (Anonymous)", "https://www.proxy-list.net/anonymous-proxies.html")
    ]

    menu_panel = Panel(
        Align.center(
            Text("TOOL ƒê√ÄO PROXY VIP", style="bold green")
        ),
        title="[bold yellow]Proxy Harvester Tool[/]",
        border_style="green",
        padding=(1, 2),
    )

    while True:
        console.print(menu_panel)
        console.print("[cyan]üåê C√°c ngu·ªìn proxy VIP c√≥ s·∫µn:[/]")
        for i, source_info in enumerate(sources):
            source_name, source_url = source_info
            console.print(f"[cyan] {i+1}.[/] [white]{source_name}[/]")

        console.print("\n[b cyan]‚öôÔ∏è Ch·ªçn ch·ª©c nƒÉng:[/b cyan]")
        console.print("[cyan] 1.[/] [white]ƒê√†o Proxy[/] [dim](HTTP & HTTPS, ch·ªçn ngu·ªìn)[/dim]")
        console.print("[cyan] 2.[/] [white]Ki·ªÉm tra Proxy[/] [dim](HTTP & HTTPS, ƒë√£ ƒë√†o)[/dim]")
        console.print("[cyan] 3.[/] [white]Tho√°t[/]")
        choice = console.input("[green]L·ª±a ch·ªçn (1-28):[/] ")

        if choice == '1':
            selected_source_indices = []
            source_input = console.input("[cyan]Nh·∫≠p s·ªë ngu·ªìn (v√≠ d·ª•: 1 2 15, 1-28, ƒë·ªÉ tr·ªëng: t·∫•t c·∫£ - [dim]c√≥ th·ªÉ ch·∫≠m h∆°n[/dim]):[/] ") 
						
            if source_input:
                if not source_input.strip():
                    console.print("[yellow]‚ö†Ô∏è  ƒêang ƒë√†o t·ª´ T·∫§T C·∫¢ c√°c ngu·ªìn. Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t nhi·ªÅu th·ªùi gian h∆°n.[/]")
                try:
                    source_indices_str = re.split(r'[,\s]+', source_input.strip()) 
										
                    for part in source_indices_str:
                        if '-' in part:
                            start, end = map(int, part.split('-'))
                            selected_source_indices.extend(range(start - 1, end))
                        else:
                            selected_source_indices.append(int(part) - 1)

                    invalid_sources = [index + 1 for index in selected_source_indices if index < 0 or index >= len(sources)]
                    selected_source_indices = [index for index in selected_source_indices if 0 <= index < len(sources)]

                    if invalid_sources:
                        console.print(f"[red]‚ùå L·ªói:[/red] Ngu·ªìn kh√¥ng h·ª£p l·ªá: [bold red]{', '.join(map(str, invalid_sources))}[/]. [yellow]B·ªè qua.[/yellow]")

                except ValueError:
                    console.print("[red]‚ùå L·ªói:[/] Vui l√≤ng nh·∫≠p s·ªë ho·∫∑c range h·ª£p l·ªá (v√≠ d·ª•: 1 2 15, 1-28).") 
										
                    continue
            else:
                console.print("[yellow]‚ö†Ô∏è  ƒêang ƒë√†o t·ª´ T·∫§T C·∫¢ c√°c ngu·ªìn. Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t nhi·ªÅu th·ªùi gian h∆°n.[/]")

            http_proxies, https_proxies = get_proxies(selected_source_indices, sources)
            display_proxy_lists(http_proxies, https_proxies)

            save_proxies_to_files(http_proxies, https_proxies, type="all")
            console.print(f"[green]‚úÖ ƒê√£ t·ª± ƒë·ªông l∆∞u proxy v√†o file[/][yellow] http_proxies.txt[/][green] v√†[/][yellow] https_proxies.txt[/][green]![/]")

            while True:
                back_to_menu = console.input("[cyan]Quay l·∫°i menu ch√≠nh? (y/n):[/] ")
                if back_to_menu.lower() == 'y':
                    break
                elif back_to_menu.lower() == 'n':
                    console.print("[cyan]üëã Tho√°t tool. H·∫πn g·∫∑p l·∫°i![/]")
                    return
                else:
                    console.print("[red]L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p 'y' ho·∫∑c 'n'.[/]")


        elif choice == '2':
            http_proxies = []
            https_proxies = []
            try:
                with open("http_proxies.txt", 'r') as f_http, open("https_proxies.txt", 'r') as f_https:
                    http_proxies = [line.strip() for line in f_http.readlines()]
                    https_proxies = [line.strip() for line in f_https.readlines()]
            except FileNotFoundError:
                console.print("[yellow]‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file proxy ƒë·ªÉ ki·ªÉm tra. Vui l√≤ng ƒë√†o proxy tr∆∞·ªõc.[/]")
                continue

            if not http_proxies and not https_proxies:
                console.print("[yellow]‚ö†Ô∏è Kh√¥ng c√≥ proxy n√†o ƒë·ªÉ ki·ªÉm tra trong file.[/]")
                continue

            live_http_proxies, dead_http_proxies = check_proxies_multithreaded(http_proxies, "http")
            live_https_proxies, dead_https_proxies = check_proxies_multithreaded(https_proxies, "https")

            console.print(Panel(Align.center(Text("K·∫øt qu·∫£ ki·ªÉm tra Proxy HTTP", style="bold green")), border_style="green", title="[bold green]üîé K·∫øt qu·∫£ HTTP Check [/]"))
            for proxy in live_http_proxies:
                console.print(f"[green]‚úÖ [LIVE - HTTP][/] [white]{proxy}[/]")
            for proxy in dead_http_proxies:
                console.print(f"[red]üíÄ [DEAD - HTTP][/] [white]{proxy}[/]")
            console.print(f"\n[green]üìä T·ªïng c·ªông HTTP s·ªëng:[/] [white bold]{len(live_http_proxies)}[/]/[white]{len(http_proxies)}[/]")

            console.print(Panel(Align.center(Text("K·∫øt qu·∫£ ki·ªÉm tra Proxy HTTPS", style="bold green")), border_style="green", title="[bold green]üîé K·∫øt qu·∫£ HTTPS Check [/]"))
            for proxy in live_https_proxies:
                console.print(f"[green]‚úÖ [LIVE - HTTPS][/] [white]{proxy}[/]")
            for proxy in dead_https_proxies:
                console.print(f"[red]üíÄ [DEAD - HTTPS][/] [white]{proxy}[/]")
            console.print(f"\n[green]üìä T·ªïng c·ªông HTTPS s·ªëng:[/] [white bold]{len(live_https_proxies)}[/]/[white]{len(https_proxies)}[/]")

            save_proxies_to_files(live_http_proxies, live_https_proxies, type="live")
            console.print(f"[green]‚úÖ ƒê√£ t·ª± ƒë·ªông l∆∞u proxy LIVE v√†o file[/][yellow] live_http_proxies.txt[/][green] v√†[/][yellow] live_https_proxies.txt[/][green]![/]")

            while True:
                back_to_menu = console.input("[cyan]Quay l·∫°i menu ch√≠nh? (y/n):[/] ")
                if back_to_menu.lower() == 'y':
                    break
                elif back_to_menu.lower() == 'n':
                    console.print("[cyan]üëã Tho√°t tool. H·∫πn g·∫∑p l·∫°i![/]")
                    return
                else:
                    console.print("[red]L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p 'y' ho·∫∑c 'n'.[/]")


        elif choice == '3':
            console.print("[cyan]üëã Tho√°t tool. H·∫πn g·∫∑p l·∫°i![/]")
            break
        else:
            console.print("[red]‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá. Vui l√≤ng ch·ªçn l·∫°i.[/]")

if __name__ == "__main__":
    main_menu()